<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Josh Nicholas</title>
    <meta name="description" content="Josh is a journalist who codes and draws">
    <link rel="stylesheet" href="/css/index.css">
    <link rel="stylesheet" href="/css/prism-base16-monokai.dark.css">
    <link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="Josh Nicholas">
    <link rel="alternate" href="/feed/feed.json" type="application/json" title="Josh Nicholas">
  </head>
  <br>
  <body>
    <header>
      <ul class="nav">
      <br>

      
      

                 

        

          <li class="nav-item"><a href="/">home</a></li> |
        


                

      
      

                 

        

          <li class="nav-item"><a href="/words/">words</a></li> |
        


                

      
      

                 

        

          <li class="nav-item"><a href="/photos/">photos</a></li> |
        


                

      
      

                 

         
          <li class="nav-item"><a href="/scribbles/">scribbles</a></li>
        


                



      </ul>
    </header>

    <main class="tmpl-post">

      <div style="margin:auto !important; max-width:700px !important;">

<h1 class="post-header"></h1>



<div style="margin:auto !important; max-width:700px !important;">

<h1><a href="/words/facts-are-cold/" class="post-header">Facts are cold</a></h1>


<p><!--kg-card-begin: html--><p>One of the most galling things about politics and policy is how ancillary &#8220;facts&#8221; and logic can be. The internet has lowered the bar to information and expertise to almost nothing, but has devalued them as well.</p><br>
<p>The utopia of <em>evidenced based policy</em> seems unlikely to arrive. Even putting aside vested interests. This is sort of related to <a href="https://joshnicholas.com/everything-is-complicated/">my previous post</a> on how fond we are of reasoning through faulty analogy. But I wonder if it is not a more fundamental issue of the coldness of facts.</p><br>
<p>This thought struck while reading <a href="https://www.worldcat.org/title/man-who-solved-the-market-how-jim-simons-launched-the-quaint-revolution/oclc/1126569133&#038;referer=brief_results">The Man Who Solved the Market</a> by Gregory Zuckerman. The book traces the story of Jim Simons and Renaissance Technologies. They were some of the pioneers of quantitative trading and have had almost unparalleled success over decades.</p><br>
<p>What separates Renaissance (at least initially) is that Jim and most of the others were mathematicians and computer scientists. Many didn&#8217;t have any experience, or even interest in, finance. Hence their goal was to create an automated trading system:</p><br>
<blockquote><p>Humans are prone to fear, greed, and outright panic, all of which tend to sow volatility in financial markets. <strong>Machines could make markets more stable, if they elbow out individuals governed by biases and emotions</strong>. And computer-driven decision-making in other fields, such as the airline industry, has generally led to fewer mistakes.</p><br>
</blockquote>
<p>But throughout the story you can feel a tug, between thus clear drive towards a fully automated system and a reticence to cede control. This was especially clear in times of turmoil and as the models became so complex and self-learning that it was all but impossible to understand why exactly a trade was being made (or reccomended).</p><br>
<blockquote><p>Then, something unexpected happened. The computerized system developed an unusual appetite for potatoes, shifting two-thirds of its cash into futures contracts on the New York Mercantile Exchange that represented millions of pounds of Maine potatoes. One day, Simons got a call from unhappy regulators at the Commodity Futures Trading Commission: Monemetrics was close to cornering the global market for these potatoes, they said, with some alarm. Simons had to stifle a giggle. Yes, the regulators were grilling him, but they had to realize Simons hadn’t meant to accumulate so many potatoes; <strong>he couldn’t even understand why his computer system was buying so many of them</strong>. Surely, the CFTC would understand that.</p><br>
<p>Soon, he and Baum had lost confidence in their system. They could see the Piggy Basket’s trades and were aware when it made and lost money, but Simons and Baum weren’t sure why the model was making its trading decisions. <strong>Maybe a computerized trading model wasn’t the way to go, after all, they decided</strong>.</p><br>
</blockquote>
<p>What&#8217;s striking is that these were all people who understood the underlying logic of such a system. That while it&#8217;s (probably) impossible to predict any particular stock or commodity, there are patterns in the data. Patterns that represent biases, mistakes and other phenomena. That these can be identified. And, given enough &#8220;bets&#8221;, they only had to be correct 50.075% of the time to make an absolute killing.</p><br>
<p>Most of them had been involved in the construction of the model. Many had previously done academic research, and even invented techniques, on which it was based. And yet they still had trouble trusting something they didn&#8217;t fully understand.</p><br>
<blockquote><p>Some rank-and-file senior scientists were upset—not so much by the losses, but because Simons had interfered with the trading system and reduced positions. Some took the decision as a personal affront, a sign of ideological weakness and a lack of conviction in their labor. “You’re dead wrong,” a senior researcher emailed Simons. <strong>“You believe in the system, or you don’t,” another scientist said, with some disgust</strong>.</p><br>
</blockquote>
<p>Of course, they were also aware of the inverse &#8211; how fallible human traders are. And that many of their own mistakes, especially early on, were the result of human intervention.</p><br>
<p>But it was obviously hard and contentious to move beyond this hill. To trust something they didn&#8217;t fully understand. The &#8220;facts&#8221; from nowhere. The truth without a good story. Maybe this is the problem with evidence based policy.</p><br>
<p><em>As always my emphasis</em></p><br>
<!--kg-card-end: html--></p>

<br>
<br>



<hr>




<br>



<h1><a href="/words/everything-is-complicated/" class="post-header">Everything is complicated</a></h1>


<p><!--kg-card-begin: html--><p>The impossibility in reading the news and pontificating about public policy is putting yourself in others&#8217; shoes. This isn&#8217;t to say we don&#8217;t do it. We all do it all the time. We&#8217;re just terrible at it.</p><br>
<p>I touched on this when <a href="https://joshnicholas.com/a-plea-for-more-humility-about-what-we-know/">writing about/reading</a> the Hidden Half. But it&#8217;s coming out again as I read a most intriguing book on social science called <a href="https://www.worldcat.org/title/everything-is-obvious-how-common-sense-fails/oclc/861071126&#038;referer=brief_results">Everything is Obvious</a> by Duncan Watts.</p><br>
<p>What I&#8217;m talking about is the tendency to try and understand why people behave a certain way. Or to dream up interventions to force them to do so. It almost always involves reasoning through analogy, which is something I&#8217;m very guilty of (just read basically anything on this blog).</p><br>
<p>This kind of reasoning plays into a false notion that behaviour is the predictable outcome of certain inputs. And, as Watts points out, even if this is so, we&#8217;re terrible at recognising and weighting all the inputs in our own decisions, let alone everyone else.</p><br>
<blockquote><p>Rationalizing human behavior, however, is precisely an exercise in simulating, in our mind’s eye, what it would be like to be the person whose behavior we are trying to understand. Only when we can imagine this simulated version of ourselves responding in the manner of the individual in question do we really feel that we have understood the behavior in question. <strong>So effortlessly can we perform this exercise of “understanding by simulation” that it rarely occurs to us to wonder how reliable it is&#8230;</strong></p><br>
</blockquote>
<blockquote><p><strong>&#8230;our mental simulations have a tendency to ignore certain types of factors that turn out to be important.</strong> The reason is that when we think about how we think, we <strong>instinctively emphasize consciously accessible costs and benefits</strong> such as those associated with motivations, preferences, and beliefs—the kinds of factors that predominate in social scientists’ models of rationality.</p><br>
</blockquote>
<p>Watts goes on to cite a laundry list of implicit factors that shape our decision making. From defaults to various kinds of priming and faulty memory and reasoning. These biases and gaps are now well known, but it&#8217;s interesting to think first about how little we understand about ourselves.</p><br>
<blockquote><p>&#8230;although it may be true that I like ice cream as a general rule, how much I like it at a particular point in time might vary considerably, depending on the time of day, the weather, how hungry I am, and how good the ice cream is that I expect to get. My decision, moreover, doesn’t depend just on how much I like ice cream, or even just the relation between how much I like it versus how much it costs. It also depends on whether or not I know the location of the nearest ice cream shop, whether or not I have been there before, how much of a rush I’m in, who I’m with and what they want, whether or not I have to go to the bank to get money, where the nearest bank is, whether or not I just saw someone else eating an ice cream, or just heard a song that reminded me of a pleasurable time when I happened to be eating an ice cream, and so on. <strong>Even in the simplest situations, the list of factors that might turn out to be relevant can get very long very quickly.</strong> And with so many factors to worry about, even very similar situations may differ in subtle ways that turn out to be important. <strong>When trying to understand—or better yet predict—individual decisions, how are we to know which of these many factors are the ones to pay attention to, and which can be safely ignored?</strong></p><br>
</blockquote>
<p>Now imagine doing this over a population.</p><br>
<p><em>As always my emphasis</em></p><br>
<!--kg-card-end: html--></p>

<br>
<br>



<hr>




<br>



<h1><a href="/words/code-is-fragile/" class="post-header">Code is fragile</a></h1>


<p><!--kg-card-begin: html--><p>Something I hadn’t expected to learn this year was that computer code spits the dummy over the slightest thing. Given a slight change, the barest deviation from what a script was expecting, the whole thing shuts down.</p><br>
<p>If you’re lucky (and have prepared ahead of time) it might throw out an error message. But mostly it sits and sulks until whatever exception to the rules you’ve given it has been fixed.</p><br>
<p>Which is partly what makes me pessimistic about things like autonomous cars. Here&#8217;s another grab from <a href="https://www.worldcat.org/title/you-look-like-a-thing-and-i-love-you/oclc/1128058352&#038;referer=brief_results">You look like a thing and I love you</a> by Janelle Shane:</p><br>
<blockquote><p>Our world is too complicated, too unexpected, <strong>too bizarre for an AI to have seen it all during training</strong>. The emus will get loose, the kids will start wearing cockroach costumes, and people will ask about giraffes even when there aren’t any present. AI will misunderstand us because it lacks the context to know what we really want it to do.</p><br>
</blockquote>
<p>I now have several scripts running every day, peppered with code asking it to pretty-please keep going if something goes wrong. It’s a tangled web of counterfactual logic, mostly dreamed up after something actually has gone wrong. Most days it makes it. But often it doesn’t.</p><br>
<p>Of course autonomous cars aren&#8217;t as bad as my hard coded logic. Part of the point of machine learning is precisely to avoid having to come up with all the steps and ass-covering required to make code tackle a complex and multifaceted problem.</p><br>
<p>But we&#8217;ve now seen so many cases where it just doesn&#8217;t work. Because the same problems apply when it comes to training the algorithms.</p><br>
<p>The real world is so much more wild and malleable than the relatively safe cyberspace my code calls home. The people tackling these problems are obviously far smarter and more experienced than me, but is that enough?</p><br>
<blockquote><p>All sorts of things could change and mess with an AI. As I mentioned in an earlier chapter, road closures or even hazards like wildfires might not deter an AI that sees only traffic from recommending what it thinks is an attractive route. Or a new kind of scooter could become popular, throwing off the hazard-detection algorithm of a self-driving car. <strong>A changing world adds to the challenge of designing an algorithm to understand it</strong>.</p><br>
</blockquote>
<p>I suspect this post will be outdated incredibly fast. But it&#8217;s also likely that our wildest technological dreams will be achieved less by computers being &#8220;smarter&#8221; and more through narrowing the problem. Making the world safer. Because code is fragile.</p><br>
<p><em>As always my emphasis</em></p><br>
<!--kg-card-end: html--></p>

<br>
<br>



<hr>




<br>



<h1><a href="/words/its-not-malice-but-incompetence-thatll-kill-us/" class="post-header">Its not malice but incompetence thatll kill us</a></h1>


<p><!--kg-card-begin: html--><p>I&#8217;ve been reading <a href="https://www.worldcat.org/title/you-look-like-a-thing-and-i-love-you/oclc/1128058352&#038;referer=brief_results">You look like a thing and I love you</a> by Janelle Shane. And, honestly, it&#8217;s some of the best skewering of Artificial Intelligence I&#8217;ve come across. But amid the funny stories of AI incompetence &#8211; only recognising sheep when they&#8217;re in fields, thinking a goat in a tree is a giraffe etc. &#8211; there&#8217;s a serious point about the impact of these limitations.</p><br>
<blockquote><p>As more of our daily lives are governed by algorithms, the quirks of AI are beginning to have consequences far beyond the merely inconvenient. Recommendation algorithms embedded in YouTube point people toward ever more polarizing content, traveling in a few short clicks from mainstream news to videos by hate groups and conspiracy theorists&#8230;</p><br>
</blockquote>
<blockquote><p>&#8230;The algorithms that make decisions about parole, loans, and resume screening <strong>are not impartial but can be just as prejudiced as the humans they’re supposed to replace—sometimes even more so</strong>. AI-powered surveillance can’t be bribed, but it also can’t raise moral objections to anything it’s asked to do. It can also make mistakes when it’s misused—or even when it’s hacked. <strong>Researchers have discovered that something as seemingly insignificant as a small sticker can make an image recognition AI think a gun is a toaster, and a low-security fingerprint reader can be fooled more than 77 percent of the time with a single master fingerprint</strong>.</p><br>
</blockquote>
<p>People are generally quick to chalk up to malice what is better explained by incompetence. The righteous outrage in the papers is full of pinstriped fat cats rather than honest mistakes and fallible processes (sometimes rightly so, but probably not as often as portrayed).</p><br>
<p>It seems our fears for the future fall into this same trap. What truly scares me about AI is generally the outcome of a perfectly running system. Skynet murderbots conquering the world. Or batallions of worker drones tilting the balance further in favour of capital and the technologically competent.</p><br>
<p>But my recent studies of machine learning, and this book specifically, make we wonder whether the true worry shouldn&#8217;t be bias and incompetence. Not a murderous bot. Rather one that just isn&#8217;t ready for prime time. In that sense the problem isn&#8217;t the technology itself, but the underlying systems that deployed it (I didn&#8217;t explicitly write &#8220;capitalism&#8221; here, but I wouldn&#8217;t fault you for reading it in 😇).</p><br>
<p>If there&#8217;s malice in the system, that&#8217;s where to find it.</p><br>
<blockquote><p>When people think of AI disaster, they think of AIs refusing orders, deciding that their best interests lie in killing all humans, or creating terminator bots. But <strong>all those disaster scenarios assume a level of critical thinking and a humanlike understanding of the world that AIs won’t be capable of for the foreseeable future</strong>. As leading machine learning researcher Andrew Ng put it, worrying about an AI takeover is like worrying about overcrowding on Mars.</p><br>
</blockquote>
<p>One of Shane&#8217;s principles of &#8220;AI weirdness&#8221; is that it does not really understand the problem you want it to solve. Another is that it will take the path of least resistance to achieve what you tell it to.</p><br>
<p>When you&#8217;re using AI to play a game this can lead to some obnoxious cheating. When you&#8217;re deploying it in the real world this can result in further entrenching bias, hierarchy and revealed preference. Often in ways we don&#8217;t understand and expect. Particularly if the implementors aren&#8217;t aware of the underlying bias in the system they are a part of.</p><br>
<blockquote><p>The problem with designing an AI to screen candidates for us: we aren’t really asking the AI to identify the best candidates. We’re asking it to identify the candidates that most resemble the ones our human hiring managers liked in the past. That might be okay if the human hiring managers made great decisions. But most US companies have a diversity problem, particularly among managers and particularly in the way that hiring managers evaluate resumes and interview candidates. All else being equal, resumes with white-male-sounding names are more likely to get interviews than those with female-and/ or minority-sounding names. 5 Even hiring managers who are female and/ or members of a minority themselves tend to unconsciously favor white male candidates. <strong>Plenty of bad and/ or outright harmful AI programs are designed by people who thought they were designing an AI to solve a problem but were unknowingly training it to do something entirely different</strong>.</p><br>
</blockquote>
<p><em></em></p><br>
<p><em>As always my emphasis</em></p><br>
<!--kg-card-end: html--></p>

<br>
<br>



<hr>




<br>




</div>

<div style="text-align:center; font-size: 0.9em";>


  <a href="/words/5/">previous</a> | 


  <a href="/words/7/">next</a>


</div>

</div>
    </main>

    <footer></footer>

    <!-- Current page: /words/6/ -->
  </body>
</html>
